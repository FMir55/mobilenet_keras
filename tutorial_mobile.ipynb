{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tutorial_mobile.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPfLQiWQV51gYgvzs/6TAx8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H3aj2KbcvfJS"},"source":["# Part 1. Gesture Recognition with MobileNet(v1)\n","In this tutorial, we are going to train a classifier that recognizes gestures. "]},{"cell_type":"markdown","metadata":{"id":"HTZfds7hzbXr"},"source":["## Import packages\n","* First of all, import all packages used in this tutorial."]},{"cell_type":"code","metadata":{"id":"EjIrZobA_wHx"},"source":["import os\n","import cv2\n","import keras\n","import random\n","import itertools\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from keras import layers\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","from sklearn.metrics import confusion_matrix\n","from IPython.display import display, Javascript, HTML, Image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z9zhXSM7yqOV"},"source":["## Function declaration\n","* Next, define all the functions used later. \n","* Just directly run the cell "]},{"cell_type":"code","metadata":{"id":"D4Ge6BCHy2_d"},"source":["def show_inference_info(iterator, confidences, predictions):\n","  iter_tmp = iterator\n","  iter_tmp.batch_size = iterator.n\n","  batchX, batchy = iter_tmp.next()\n","\n","  # randomly pick 9 results to show \n","  fig = plt.figure(figsize=(10, 10))\n","  fig.suptitle('Predictions with confidence:', fontsize=16)\n","  for i in range(9):\n","    idx = random.choice(list(range(iter_tmp.n)))\n","    confidence, y_gt, y_pred = confidences[idx], batchy[idx].astype('int'), predictions[idx]\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow((batchX[idx]*255).astype(\"uint8\"))\n","    title = '{} ({:.2f} %) Label: {}.'.format(class_names[y_pred], \n","                                              100 * np.max(confidence),\n","                                              class_names[y_gt])\n","    plt.title(title, color='red' if y_pred != y_gt else 'green')\n","    plt.axis(\"off\")\n","\n","def show_train_info(history, epochs):\n","  fig = plt.figure(figsize=(8, 8))\n","  fig.suptitle('KPI & Objective function', fontsize = 16)\n","\n","  plt.subplot(1, 2, 1)\n","  plt.plot(range(epochs), history.history['accuracy'], label='Training Accuracy')\n","  plt.plot(range(epochs), history.history['val_accuracy'], label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.title('Training and Validation Accuracy')\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(range(epochs), history.history['loss'], label='Training Loss')\n","  plt.plot(range(epochs), history.history['val_loss'], label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.title('Training and Validation Loss')\n","  plt.show()\n","  \n","def show_data_info(iterator):\n","  batchX, batchy = iterator.next()\n","  print('Input batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n","  print('Output batch shape=%s, min=%d, max=%d' % (batchy.shape, batchy.min(), batchy.max()))\n","  print('Class_name: {}' .format(train_it.class_indices))\n","  print('Lable sample: {}' .format(batchy))\n","  print('One-Hot sample:')\n","  print(keras.utils.to_categorical(batchy))\n","\n","  # Show augmented image samples\n","  fig = plt.figure(figsize=(10, 10))\n","  fig.suptitle('Augmented image samples', fontsize=16)\n","  for i in range(9):\n","      ax = plt.subplot(3, 3, i + 1)\n","      plt.imshow((batchX[i]*255).astype(\"uint8\"))\n","      plt.title(class_names[int(batchy[i])])\n","      plt.axis(\"off\")\n","\n","def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n","  # used for plot confusion matrix\n","  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","  plt.title('Confusion matrix')\n","  plt.colorbar()\n","  tick_marks = np.arange(len(classes))\n","  plt.xticks(tick_marks, classes, rotation=45)\n","  plt.yticks(tick_marks, classes)\n","\n","  thresh = cm.max() / 2.\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","      plt.text(j, i, cm[i, j],\n","                horizontalalignment=\"center\",\n","                color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","  plt.tight_layout()\n","  plt.ylabel('True label')\n","  plt.xlabel('Predicted label')\n","\n","def record_video(filename='video.mp4'):\n","  js = Javascript(\"\"\"\n","    async function recordVideo() {\n","      const options = { mimeType: \"video/webm; codecs=vp9\" };\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      const stopCapture = document.createElement(\"button\");\n","      capture.textContent = \"Start Recording\";\n","      capture.style.background = \"green\";\n","      capture.style.color = \"white\";\n","\n","      stopCapture.textContent = \"Stop Recording\";\n","      stopCapture.style.background = \"red\";\n","      stopCapture.style.color = \"white\";\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      const recordingVid = document.createElement(\"video\");\n","      video.style.display = 'block';\n","\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      let recorder = new MediaRecorder(stream, options);\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      await new Promise((resolve) => {\n","        capture.onclick = resolve;\n","      });\n","      recorder.start();\n","      capture.replaceWith(stopCapture);\n","      // use a promise to tell it to stop recording\n","      await new Promise((resolve) => stopCapture.onclick = resolve);\n","      recorder.stop();\n","\n","      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n","      let arrBuff = await recData.data.arrayBuffer();\n","      \n","      // stop the stream and remove the video element\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","\n","      let binaryString = \"\";\n","      let bytes = new Uint8Array(arrBuff);\n","      bytes.forEach((byte) => {\n","        binaryString += String.fromCharCode(byte);\n","      })\n","      return btoa(binaryString);\n","    }\n","    \"\"\")\n","  try:\n","    display(js)\n","    data = eval_js('recordVideo({})')\n","    binary = b64decode(data)\n","    with open(filename, \"wb\") as video_file:\n","      video_file.write(binary)\n","    print(\n","        f\"Finished recording video. Saved binary in directory: {filename}\"\n","    )\n","  except Exception as err:\n","      # In case any exceptions arise\n","      print(str(err))\n","  return filename\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoHeight;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, \n","                                        (video.videoWidth - video.videoHeight)/2, 0, video.videoHeight, video.videoHeight,\n","                                        0, 0, video.videoHeight, video.videoHeight);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  try:\n","    display(js)\n","    data = eval_js('takePhoto({})'.format(quality))\n","    binary = b64decode(data.split(',')[1])\n","    with open(filename, 'wb') as f:\n","      f.write(binary)\n","    print('Saved to {}'.format(filename))\n","    \n","    # Show the image which was just taken.\n","    display(Image(filename))\n","    \n","  except Exception as err:\n","    print(str(err))\n","\n","def create_directory(dir):\n","  if os.path.isdir(dir) is False:\n","    os.mkdir(dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqa7pMbbxQhZ"},"source":["## Dataset: [Hand Gesture Datasets](https://lttm.dei.unipd.it/downloads/gesture/)\n","![](https://drive.google.com/uc?export=view&id=1K5NWR8S3XITwZ4fwQpJ1IaW51bbzaWv_)\n","There are totally 11 different kinds of gestures(classes). In this tutorial, we only pick 4 of them . \\\n","*   4 classes\n","  1.   G0\n","  2.   G2\n","  3.   G3\n","  4.   G5 \\\n","\n","The customized dataset is prepared and available on google drive. Let's download it then unzip. "]},{"cell_type":"code","metadata":{"id":"2LA_jgi5_x28"},"source":["!wget --no-check-certificate 'https://drive.google.com/u/0/uc?id=13p3HrbooMoyqJ261RrhYtF954ZwV6WqD&export=download' -O senz_light.zip\n","if os.path.isdir('senz_light') is False:\n","  !unzip senz_light.zip\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xdyf7wt1EjM"},"source":["## Data Preprocessing\n","*   For MobileNet, the input must be square images with one of the following border width: \n","  - [128, 160, 192, 224]\n","*   Data Augmentation\n","  - [keras.preprocessing.image.ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n","  - [keras.layers.experimental.preprocessing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)"]},{"cell_type":"code","metadata":{"id":"5mDkfpkN_x54"},"source":["size, batch = 128, 16\n","assert size in [128, 160, 192, 224]\n","path_train, path_val, path_test = ('senz_light/train', 'senz_light/val', 'senz_light/test')\n","\n","#  Using the data Augmentation in traning data\n","generator_train = keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255,\n","                                  shear_range=0.2,\n","                                  zoom_range=0.2,\n","                                  width_shift_range=0.2,\n","                                  height_shift_range= 0.1,\n","                                  rotation_range = 30)\n","generator_inf = keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n","\n","train_it = generator_train.flow_from_directory(path_train,\n","                                              target_size=(size, size),\n","                                              color_mode = 'rgb',\n","                                              batch_size = batch,\n","                                              class_mode= 'binary')\n","\n","validation_it = generator_inf.flow_from_directory(path_val,\n","                                                  target_size=(size, size),\n","                                                  color_mode = 'rgb',\n","                                                  batch_size = batch,\n","                                                  class_mode= 'binary',\n","                                                  shuffle = False)\n","\n","test_it = generator_inf.flow_from_directory(path_test,\n","                                            target_size=(size, size),\n","                                            color_mode = 'rgb',\n","                                            batch_size = batch,\n","                                            class_mode= 'binary',\n","                                            shuffle = False)\n","\n","class_names = list(train_it.class_indices.keys())\n","num_classes = len(class_names)\n","\n","# confirm the iterator works\n","show_data_info(train_it)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30ZbBYmwHvaw"},"source":["## Ways to create a keras model\n","* ([keras.applications](https://keras.io/api/applications/))\n","* Sequential\n","* Functional\n","* [Modle Subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)"]},{"cell_type":"markdown","metadata":{"id":"xrHpDh9f1LpR"},"source":["## MobileNet-v1\n","[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n","Applications](https://arxiv.org/pdf/1704.04861.pdf) \\\n","* Standard convolution --> [Depthwise separable convolution](https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-mobilenet-depthwise-separable-convolution-f1ed016b3467) \\\n","![](https://drive.google.com/uc?export=view&id=1bzmYbgk33IpW8EmSWSZeh_Cu159N6P2o)\n","* Depthwise separable convolution = Depthwise convolution + Pointwise convolution \\\n","![](https://miro.medium.com/max/1838/1*e_oU-f6hX4FSSD-1OukP6Q.png)\n","* Network Structure(size=224, alpha=1.0) \\\n","![](https://drive.google.com/uc?export=view&id=1vGVELlYIKtHyKIm7Qa52qoYHYf3I7NhF)\n","* Instead of (size=224, alpha=1.0), we choose (size=128, alpha=0.25) in this case.\n"," - **size**: *Resolution* in **[0.25, 0.5, 0.75, 1.0]**\n"," - **alpha**: *Width Multiplier* in **[128, 144, 160, 192]**\n","![](https://drive.google.com/uc?export=view&id=1jB7DgWm4DSNSb06lSWAhOYJCbJjINpiL)\n"]},{"cell_type":"code","metadata":{"id":"6VZ2QmZNCYU_"},"source":["class MobileNet():\n","  def __init__(self, size=128, alpha=0.25, classes=1000):\n","    assert alpha in [0.25, 0.5, 0.75, 1.0]\n","\n","    self.size = size\n","    self.alpha = alpha\n","    self.classes = classes\n","    self.img_input = layers.Input(shape=(size, size, 3))\n","    self.model = self.build_model()\n","    self.load_weights()\n","    self.add_dense_layer()\n","\n","  def get_model(self):\n","    return self.model\n","\n","  def add_dense_layer(self):\n","    x = self.model.layers[-1].output\n","    x = layers.Dense(self.classes)(x)\n","    x = layers.Softmax(name='softmax')(x)\n","    self.model = keras.Model(inputs=self.img_input, outputs= x, \n","                             name='mobilenet_%0.2f_%s' % (self.alpha, self.size))\n","\n","  def build_model(self):\n","    x = self._conv_block(self.img_input, 32, strides=(2, 2))\n","    x = self._depthwise_conv_block(x, 64, block_id=1)\n","\n","    x = self._depthwise_conv_block(x, 128, strides=(2, 2), block_id=2)\n","    x = self._depthwise_conv_block(x, 128, block_id=3)\n","\n","    x = self._depthwise_conv_block(x, 256, strides=(2, 2), block_id=4)\n","    x = self._depthwise_conv_block(x, 256, block_id=5)\n","\n","    x = self._depthwise_conv_block(x, 512, strides=(2, 2), block_id=6)\n","    x = self._depthwise_conv_block(x, 512, block_id=7)\n","    x = self._depthwise_conv_block(x, 512, block_id=8)\n","    x = self._depthwise_conv_block(x, 512, block_id=9)\n","    x = self._depthwise_conv_block(x, 512, block_id=10)\n","    x = self._depthwise_conv_block(x, 512, block_id=11)\n","\n","    x = self._depthwise_conv_block(x, 1024, strides=(2, 2), block_id=12)\n","    x = self._depthwise_conv_block(x, 1024, block_id=13)\n","\n","    x = layers.GlobalAveragePooling2D()(x)\n","\n","    # Create model.\n","    return keras.models.Model(self.img_input, x, name='mobilenet_%0.2f_%s' % (self.alpha, self.size))\n","\n","  def load_weights(self):\n","    BASE_WEIGHT_PATH = ('https://github.com/fchollet/deep-learning-models/'\n","                    'releases/download/v0.6/')\n","    # Load weights.\n","    if self.alpha == 1.0:\n","        alpha_text = '1_0'\n","    elif self.alpha == 0.75:\n","        alpha_text = '7_5'\n","    elif self.alpha == 0.50:\n","        alpha_text = '5_0'\n","    else:\n","        alpha_text = '2_5'\n","\n","    model_name = 'mobilenet_%s_%d_tf_no_top.h5' % (alpha_text, size)\n","    weight_path = BASE_WEIGHT_PATH + model_name\n","    weights_path = keras.utils.get_file(model_name, weight_path, cache_subdir='models')\n","    self.model.load_weights(weights_path)\n","\n","  def _conv_block(self, x, filters, kernel=(3, 3), strides=(1, 1)):\n","    filters = int(filters * self.alpha)\n","    x = layers.ZeroPadding2D(padding=((0, 1), (0, 1)), name='conv1_pad')(x)\n","    x = layers.Conv2D(filters, kernel,\n","                      padding='valid',\n","                      use_bias=False,\n","                      strides=strides,\n","                      name='conv1')(x)\n","    x = layers.BatchNormalization(axis=-1, name='conv1_bn')(x)\n","    return layers.ReLU(6., name='conv1_relu')(x)\n","\n","  def _depthwise_conv_block(self, x, pointwise_conv_filters, strides=(1, 1), block_id=1):\n","    pointwise_conv_filters = int(pointwise_conv_filters * self.alpha)\n","\n","    x = layers.ZeroPadding2D(((0, 1), (0, 1)), name='conv_pad_%d' % block_id)(x) if strides != (1, 1) else x\n","    x = layers.DepthwiseConv2D((3, 3),\n","                               padding='same' if strides == (1, 1) else 'valid',\n","                               depth_multiplier=1,\n","                               strides=strides,\n","                               use_bias=False,\n","                               name='conv_dw_%d' % block_id)(x)\n","    x = layers.BatchNormalization(axis=-1, name='conv_dw_%d_bn' % block_id)(x)\n","    x = layers.ReLU(6., name='conv_dw_%d_relu' % block_id)(x)\n","\n","    x = layers.Conv2D(pointwise_conv_filters, (1, 1),\n","                      padding='same',\n","                      use_bias=False,\n","                      strides=(1, 1),\n","                      name='conv_pw_%d' % block_id)(x)\n","    x = layers.BatchNormalization(axis=-1, name='conv_pw_%d_bn' % block_id)(x)\n","    return layers.ReLU(6., name='conv_pw_%d_relu' % block_id)(x)\n","\n","model = MobileNet(size =size, alpha=0.25, classes = num_classes).get_model()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKpyct9iLjd3"},"source":["## An alternative way: keras.applacations API\n","[Keras Applications](https://keras.io/api/applications/) \\\n","Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. \\\n","[tf.keras.applications.MobileNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNet) \\\n","Instantiates the MobileNet architecture.\n"]},{"cell_type":"code","metadata":{"id":"bUn1bzdYLjmL"},"source":["'''\n","mobile = keras.applications.mobilenet.MobileNet(input_shape=(size, size, 3), alpha=0.25)\n","x = mobile.layers[-6].output\n","predictions = layers.Dense(num_classes, activation='softmax')(x)\n","model = keras.Model(inputs=mobile.input, outputs= predictions)\n","model.summary()\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3MDsqQnf2AMW"},"source":["## Train & Validation\n","* Note that validation data is only used for inference, only training data \n","influences the update of trainable variables.\n","* [Optimizer](https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%BA%8C-sgd-adagrad-momentum-rmsprop-adam-optimizer-e331ef3cf5cf): Though the origin apply RMSprop, we use Adam with learning rate 0.0002. \\\n","* Loss function: [CrossEntropyLoss](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb) \n","  - Why not apply **error rate**(1-accuracy) as objective function?\n","* Epoch: Number of times that model traverses the whole training data"]},{"cell_type":"code","metadata":{"id":"iaWvgOyl_yA3"},"source":["model.compile(optimizer=keras.optimizers.Adam(lr=0.0002),\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=['accuracy'])\n","\n","epochs = 12\n","history = model.fit(train_it,\n","                    validation_data=validation_it,\n","                    steps_per_epoch= train_it.n // batch,\n","                    validation_steps= validation_it.n // batch,\n","                    epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO50hUjp3V-T"},"source":["You will get about 95% validation accuracy if everything works fine. \\\n","Let's check the improvement during the training procedure. "]},{"cell_type":"code","metadata":{"id":"5S3-woha_yEL"},"source":["show_train_info(history, epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpK9josp5BqT"},"source":["## Evaluate & Inference on test set\n","We have already trained the model on our customized dataset. \\\n","Now let's see how it works on test set (Unseen data)."]},{"cell_type":"code","metadata":{"id":"wMIH8fHC_x_E"},"source":["#Evaluate on test set\n","model.evaluate(test_it)\n","\n","# inference on test set\n","confidences = model.predict(test_it)\n","predictions = confidences.argmax(axis=1)\n","\n","# draw confusion matrix of the entire test set\n","cm = confusion_matrix(y_true=test_it.classes, y_pred = predictions)\n","plot_confusion_matrix(cm=cm, classes=class_names)\n","\n","show_inference_info(test_it, confidences, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkbylTfYsRAQ"},"source":["## Confidence score: [Softmax](https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60)\n","![](https://drive.google.com/uc?export=view&id=15tZjd6hoAPA22B3I4J7tyLCNuPY6K2CC)\n","![](https://miro.medium.com/max/1050/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"hSFz2n9TmgDd"},"source":["# Part 2: Implementation on instant captured images\n","* In this part, we are going to train on the data that generated by ourselves.\n","* For each of classes, there will be frames of images extracted from the corresponding captured video. \n","* **Try & Error**: Create your own classifier."]},{"cell_type":"markdown","metadata":{"id":"dtfTKL396TOh"},"source":["## Data collection\n","*  Let's start from 2 classes: one video record for each class.\n","*  You can try more than 2 classes if you want. (Just modify variable **num_classes**).\n","*  For each class video, **the record duration more than 10 seconds is recommended.** \n","  * Sufficient samples for more variation\n","  * Avoid overfitting"]},{"cell_type":"code","metadata":{"id":"hSOF0c12bpdN"},"source":["capture_dir_pic, capture_dir_vid = 'captured_pictures', 'captured_videos'\n","!rm -rf {capture_dir_pic}\n","!rm -rf {capture_dir_vid}\n","create_directory(capture_dir_pic)\n","create_directory(capture_dir_vid)\n","\n","num_classes =  2\n","video_paths = []\n","for i in range(num_classes):\n","  filename = os.path.join(capture_dir_vid, 'video_class{}.mp4'.format(i))\n","  print(\"Click button 'Start Recording' to start capturing data for class{}\".format(i))\n","  video_paths.append(record_video(filename=filename)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLJgNqyuK-CM"},"source":["## Conversion & Central crop\n","* We have prepared class videos, now let's extract image frames from them.  \n","* Note that only square images are available for MobileNet. \n","  * However, the size of captured video is 480*720.\n","  * Therefore, we apply central crop then get 480*480 square samples.\n","* Both format conversion and central crop can well done by [ffmpeg](https://ffmpeg.org/) with only one line."]},{"cell_type":"code","metadata":{"id":"Kvx6lut3gkU3"},"source":["for i, video_path in enumerate(video_paths):\n","  # create directory & name the file\n","  class_folder = os.path.join(capture_dir_pic, 'class_{}'.format(i))\n","  !rm -rf {class_folder}\n","  create_directory(class_folder)\n","  out_jpgs = os.path.join(class_folder, '%d.jpg')\n","  print('Video frames for class{}:'.format(i))\n","\n","  # Review saved videos\n","  video_file = open(video_path, \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  display(HTML(f\"\"\"<video width=300 controls><source src=\"{video_url}\"></video>\"\"\"))\n","\n","  # crop the frames during converting mp4 to jpgs\n","  vcap = cv2.VideoCapture(video_path)\n","  h, w = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT), vcap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","  !ffmpeg -i {video_path} -filter:v \"crop={h}:{h}:{(w-h)//2}:0\" -vf fps=20 -crf 19 {out_jpgs}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yXYUP4xxMYYj"},"source":["## Dataset split & preprocessing\n","* Train:Validation = 8:2 or you can choose your own **validation_split**.\n","* Make sure that the augmentation settings make sense toward your data.\n","  * Horizontal flip\n","  * Vertical flip\n","  * Rotation"]},{"cell_type":"code","metadata":{"id":"4ofE2o5yyXzs"},"source":["generator_train = keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255,\n","                                                                validation_split=0.2,\n","                                                                shear_range=0.2,\n","                                                                zoom_range=0.2,\n","                                                                width_shift_range=0.1,\n","                                                                height_shift_range= 0.1,\n","                                                                rotation_range = 30)\n","generator_val = keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255,\n","                                                             validation_split=0.2)\n","\n","iterator_train = generator_train.flow_from_directory(capture_dir_pic,\n","                                                    target_size=(size, size),\n","                                                    color_mode = 'rgb',\n","                                                    batch_size= batch,\n","                                                    class_mode= 'binary',\n","                                                    subset=\"training\",\n","                                                    seed = 123)\n","\n","iterator_val = generator_val.flow_from_directory(capture_dir_pic,\n","                                                  target_size=(size, size),\n","                                                  color_mode = 'rgb',\n","                                                  batch_size= batch,\n","                                                  class_mode= 'binary',\n","                                                  subset=\"validation\",\n","                                                  shuffle = False,\n","                                                  seed = 123)\n","\n","# confirm the iterator works\n","show_data_info(iterator_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgGY48c5RBKA"},"source":["* This time we get MobileNet from keras API  "]},{"cell_type":"code","metadata":{"id":"n7n1uz9cYVdC"},"source":["mobile = keras.applications.mobilenet.MobileNet(input_shape=(size, size, 3), alpha=0.25)\n","x = mobile.layers[-6].output\n","predictions = layers.Dense(num_classes, activation='softmax')(x)\n","model = keras.Model(inputs=mobile.input, outputs= predictions)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TYUfrImCXpwM"},"source":["## Start training\n","* Feel free to tune your own **epochs** and **learning rate(lr)**.\n","* Various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) are available to choose. (Optional)"]},{"cell_type":"code","metadata":{"id":"3fBxvl-IC_EA"},"source":["model.compile(optimizer=keras.optimizers.Adam(lr=0.0002),\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=['accuracy'])\n","\n","epochs = 10\n","history = model.fit(iterator_train,\n","                    validation_data=iterator_val,\n","                    steps_per_epoch= iterator_train.n // batch,\n","                    validation_steps= iterator_val.n // batch,\n","                    epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDRRYzhTVe7r"},"source":["## Review the training procedure\n","* Check if the model is well trained.  \n","* If not, figure out the problem and try again.  "]},{"cell_type":"code","metadata":{"id":"hZ7WALMBIjMC"},"source":["show_train_info(history, epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aX5jG216Vqp9"},"source":["## Inference on validation set "]},{"cell_type":"code","metadata":{"id":"VHnu5GJiIjTv"},"source":["# inference on test set\n","confidences = model.predict(iterator_val)\n","predictions = confidences.argmax(axis=1)\n","\n","# draw confusion matrix of the entire test set\n","cm = confusion_matrix(y_true=iterator_val.classes, y_pred = predictions)\n","plot_confusion_matrix(cm=cm, classes=class_names)\n","\n","show_inference_info(iterator_val, confidences, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLe9ybpmV1YI"},"source":["## Try with a new unseen picture"]},{"cell_type":"code","metadata":{"id":"voBtpvaIIjZW"},"source":["filename = 'photo.jpg'\n","print(\"Capture the test image by clicking 'Capture' button.\")\n","take_photo(filename)\n","\n","img = cv2.imread(filename)\n","img = cv2.resize(img, (size, size), interpolation=cv2.INTER_CUBIC) #resize\n","img = img.astype('float32') /255 # rescale\n","img_in = tf.expand_dims(img, axis=0) # expand batch dimension\n","\n","scores = model.predict(img_in)\n","max_score = tf.nn.softmax(scores[0])\n","print(\n","    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","    .format(class_names[np.argmax(max_score)], 100 * np.max(max_score))\n",")"],"execution_count":null,"outputs":[]}]}